{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52hJ1FoEkCw-"
      },
      "source": [
        "#multi GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d1Kt4QVkI7w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dense\n",
        "from keras.optimizers import Adam,Nadam,RMSprop,SGD\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "tf.keras.utils.set_random_seed(2023)\n",
        "\n",
        "# Load data from CSV file\n",
        "economic_df=pd.read_csv(\"drive/My Drive/Master Thesis/lCubicImputation_df.csv\")\n",
        "economic_df= economic_df.apply(lambda x: np.log(x) if np.issubdtype(x.dtype, np.number) else x)\n",
        "data=economic_df\n",
        "# Select the input features and the target feature for prediction\n",
        "#input_features = ['b3']\n",
        "input_features =['b5']\n",
        "target_feature = 'b5' # The feature you want to predict\n",
        "\n",
        "# Prepare data for training\n",
        "look_back = 3 # Number of previous time steps to consider for prediction-REMAIN 2 CATEGORY FOR 10\n",
        "\n",
        "X, y = [], []\n",
        "for i in range(len(data) - look_back):\n",
        "    X.append(data[input_features].iloc[i:i+look_back].values)\n",
        "    y.append(data[target_feature].iloc[i+look_back])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(1/2), shuffle=False)\n",
        "\n",
        "# Define hyperparameter values to try\n",
        "\n",
        "opt_list=[RMSprop,Adam,Nadam,SGD]\n",
        "units_list = [64,128,256,512,1024]\n",
        "batch_size_list = [4,8,16,32,64]\n",
        "epochs_list = [50,100,150]\n",
        "activation=['linear']\n",
        "\n",
        "#act='linear'\n",
        "val_r2=[]\n",
        "train_r2=[]\n",
        "test_r2=[]\n",
        "train_mse=[]\n",
        "test_mse=[]\n",
        "hparams=[]\n",
        "# Loop through hyperparameter combinations\n",
        "for units in units_list:\n",
        "    for batch_size in batch_size_list:\n",
        "        for epochs in epochs_list:\n",
        "          for opt in opt_list:\n",
        "            for act in activation:\n",
        "            # Build the GRU model\n",
        "              model = Sequential()\n",
        "              model.add(GRU(units=units, input_shape=(look_back, len(input_features))))\n",
        "              model.add(Dense(1, activation=act))\n",
        "              optimizer = opt(learning_rate=0.001)\n",
        "              model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "\n",
        "              # Train the model with validation set\n",
        "              history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)\n",
        "\n",
        "              # Evaluate the model on the validation set\n",
        "              y_val_pred = model.predict(X_val)\n",
        "              r2_val = r2_score(y_val, y_val_pred)\n",
        "              val_r2.append(r2_val)\n",
        "\n",
        "              # Build the final model with the  hyperparameters\n",
        "              final_model = Sequential()\n",
        "              final_model.add(GRU(units=units, input_shape=(look_back, len(input_features))))\n",
        "              final_model.add(Dense(1, activation=act))\n",
        "              optimizer = opt(learning_rate=0.001)\n",
        "              final_model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
        "              final_model.fit(np.concatenate((X_train, X_val)), np.concatenate((y_train, y_val)), epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "              # Evaluate the final model on the test set\n",
        "              y_test_pred = final_model.predict(X_test)\n",
        "              mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "              test_mse.append(mse_test)\n",
        "              r2_test = r2_score(y_test, y_test_pred)\n",
        "              test_r2.append(r2_test)\n",
        "              # Evaluate the final model on the training set\n",
        "              y_train_pred = final_model.predict(X_train)\n",
        "              mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "              train_mse.append(mse_train)\n",
        "              r2_train = r2_score(y_train, y_train_pred)\n",
        "              train_r2.append(r2_train)\n",
        "              hparams.append((look_back,units,batch_size,epochs,act,opt))\n",
        "\n",
        "result_df=pd.DataFrame({'val_r2':val_r2,'train_mse':train_mse,'test_mse':test_mse,'train_r2':train_r2,'test_r2':test_r2,'hparams':hparams})\n",
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k2KWMTXtPqO"
      },
      "outputs": [],
      "source": [
        "result_df.to_csv('GRU-B5-multi.csv',index=False)\n",
        "files.download('GRU-B5-multi.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPjdv3fJkXQo"
      },
      "source": [
        "First Without b1 and b3, look for best multivariate model, then add one of b1 and b3(any one that have bigger corr with out put, or ,best 'only' model found for that)\n",
        "after all, without using log transform predict real data or normalized of them\n",
        "only\n",
        "multi with b1,b3\n",
        "multi with all\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}