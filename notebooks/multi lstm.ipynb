{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["TLLjImVEHg9e","Q-wBPnlTHk-f","2HT-rX6IHrBG"],"authorship_tag":"ABX9TyMplwB2fdZeSMnd5GRI8xD7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#IMPORT"],"metadata":{"id":"TLLjImVEHg9e"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Duaw6K7iIeNw","executionInfo":{"status":"ok","timestamp":1690738041677,"user_tz":-210,"elapsed":3175,"user":{"displayName":"samira sheikha","userId":"14992994802119851005"}},"outputId":"ef092953-1ac3-464b-b13f-c01cace4b9d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["['b2',\n"," 'b4',\n"," 'b5',\n"," 'b6',\n"," 'b7',\n"," 'b8',\n"," 'b9',\n"," 'b10',\n"," 'b14',\n"," 'b17',\n"," 'b18',\n"," 'B-23',\n"," 'B-24']"]},"metadata":{},"execution_count":1}],"source":["from google.colab import files\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","import numpy as np\n","economic_df=pd.read_csv(\"drive/My Drive/Master Thesis/Imputation/Filled/lCubicImputation_df.csv\")\n","economic_df= economic_df.apply(lambda x: np.log(x) if np.issubdtype(x.dtype, np.number) else x)     #if you want to get LN of data inplace\n","economic_df.drop(columns=['Date','b1','b3'], inplace=True)\n","f=[col for col in economic_df.columns]\n","f"]},{"cell_type":"markdown","source":["#1 Feature"],"metadata":{"id":"Q-wBPnlTHk-f"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score,mean_squared_error\n","import tensorflow as tf\n","from keras.preprocessing.sequence import TimeseriesGenerator\n","\n","tf.keras.utils.set_random_seed(2023)\n","df=economic_df\n","# Modify the list of columns accordingly to include all your features\n","features_columns = [col for col in df.columns]\n","\n","# Separate features and target variable (if you have one) in your dataset\n","X = df[features_columns].values\n","\n","y = df['b2'].values # Replace 'target' with the name of your target column\n","\n","# Split the data into train, test, and validation sets\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n","X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=(2/3),shuffle=False)\n","print(len(X_train),len(X_val),len(X_test))\n","# Reshape input data to fit the LSTM model\n","num_features = len(features_columns)\n","\n","# Grid search hyperparameters\n","best_r_squared = -np.inf\n","best_lstm_units = 0\n","best_epochs = 0\n","best_batch_size = 0\n","best_time_steps = 0\n","\n","# lstm_units_list = [32, 64, 128]\n","# epochs_list = [50, 100, 150]\n","# batch_size_list = [16, 32, 64]\n","# time_steps_list = [3, 5, 7]\n","lstm_units_list = [64]\n","epochs_list = [50]\n","batch_size_list = [32]\n","time_steps_list = [10]\n","# X_train.shape[0]\n","for lstm_units in lstm_units_list:\n","    for epochs in epochs_list:\n","        for batch_size in batch_size_list:\n","            for time_steps in time_steps_list:\n","                print(f\"Training LSTM units: {lstm_units}, Epochs: {epochs}, Batch size: {batch_size}, Time steps: {time_steps}\")\n","\n","                # Reshape input data to fit the LSTM model\n","                X_train_lstm = TimeseriesGenerator(X_train, y_train, length=time_steps, sampling_rate=1)\n","                X_val_lstm = TimeseriesGenerator(X_val, y_val, length=time_steps, sampling_rate=1)\n","\n","                # Create the LSTM model\n","                model = Sequential()\n","                model.add(LSTM(lstm_units, activation='sigmoid', input_shape=(time_steps, num_features)))\n","                model.add(Dense(1)) # Output layer with one node for regression, adjust for classification\n","\n","                # Compile the model\n","                model.compile(optimizer='Nadam', loss='mse') # Use appropriate loss for your task\n","\n","                # Train the model without shuffling\n","                model.fit(X_train_lstm, epochs=epochs,batch_size=batch_size, shuffle=False)\n","\n","                # Evaluate the model on the validation set\n","                y_val_pred = model.predict(X_val_lstm)\n","                r_squared = r2_score(y_val[time_steps:], y_val_pred)\n","                print(f\"Validation R-squared: {r_squared}\")\n","\n","                # Update best hyperparameters if necessary\n","                if r_squared > best_r_squared:\n","                    best_r_squared = r_squared\n","                    best_lstm_units = lstm_units\n","                    best_epochs = epochs\n","                    best_batch_size = batch_size\n","                    best_time_steps = time_steps\n","\n","# Train the best model on the combined training and validation set\n","X_train_combined = np.concatenate((X_train, X_val), axis=0)\n","y_train_combined = np.concatenate((y_train, y_val), axis=0)\n","X_train_combined_lstm = TimeseriesGenerator(X_train_combined, y_train_combined , length=best_time_steps, sampling_rate=1)\n","\n","model = Sequential()\n","model.add(LSTM(best_lstm_units, activation='sigmoid', input_shape=(best_time_steps, num_features)))\n","model.add(Dense(1)) # Output layer with one node for regression, adjust for classification\n","model.compile(optimizer='Nadam', loss='mse')\n","model.fit(X_train_combined_lstm, epochs=best_epochs, batch_size=best_batch_size, shuffle=False)\n","\n","# Evaluate the best model on the test set\n","X_test_lstm = TimeseriesGenerator(X_test, y_test , length=best_time_steps, sampling_rate=1)\n","y_test_pred = model.predict(X_test_lstm)\n","test_r_squared = r2_score(y_test[best_time_steps:], y_test_pred)\n","test_mse = mean_squared_error(y_test[best_time_steps:], y_test_pred)\n","print(f\"\\nBest hyperparameters: LSTM units: {best_lstm_units}, Epochs: {best_epochs}, Batch size: {best_batch_size}, Time steps: {best_time_steps}\")\n","print(f\"Test R-squared with best hyperparameters: {test_r_squared}\")\n","print(f\"Test mse with best hyperparameters: {test_mse}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYi6mVX_bM1R","executionInfo":{"status":"ok","timestamp":1690737969066,"user_tz":-210,"elapsed":28730,"user":{"displayName":"samira sheikha","userId":"14992994802119851005"}},"outputId":"bd9daa43-8028-45f1-b962-918ab354de98"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["117 34 17\n","Training LSTM units: 64, Epochs: 50, Batch size: 32, Time steps: 10\n","Epoch 1/50\n","1/1 [==============================] - 8s 8s/step - loss: 4.1632\n","Epoch 2/50\n","1/1 [==============================] - 0s 82ms/step - loss: 3.6036\n","Epoch 3/50\n","1/1 [==============================] - 0s 65ms/step - loss: 3.2837\n","Epoch 4/50\n","1/1 [==============================] - 0s 64ms/step - loss: 2.9815\n","Epoch 5/50\n","1/1 [==============================] - 0s 34ms/step - loss: 2.6979\n","Epoch 6/50\n","1/1 [==============================] - 0s 36ms/step - loss: 2.4357\n","Epoch 7/50\n","1/1 [==============================] - 0s 34ms/step - loss: 2.1971\n","Epoch 8/50\n","1/1 [==============================] - 0s 36ms/step - loss: 1.9839\n","Epoch 9/50\n","1/1 [==============================] - 0s 35ms/step - loss: 1.7973\n","Epoch 10/50\n","1/1 [==============================] - 0s 35ms/step - loss: 1.6375\n","Epoch 11/50\n","1/1 [==============================] - 0s 49ms/step - loss: 1.5041\n","Epoch 12/50\n","1/1 [==============================] - 0s 34ms/step - loss: 1.3956\n","Epoch 13/50\n","1/1 [==============================] - 0s 39ms/step - loss: 1.3094\n","Epoch 14/50\n","1/1 [==============================] - 0s 36ms/step - loss: 1.2422\n","Epoch 15/50\n","1/1 [==============================] - 0s 37ms/step - loss: 1.1899\n","Epoch 16/50\n","1/1 [==============================] - 0s 34ms/step - loss: 1.1483\n","Epoch 17/50\n","1/1 [==============================] - 0s 37ms/step - loss: 1.1132\n","Epoch 18/50\n","1/1 [==============================] - 0s 42ms/step - loss: 1.0810\n","Epoch 19/50\n","1/1 [==============================] - 0s 37ms/step - loss: 1.0490\n","Epoch 20/50\n","1/1 [==============================] - 0s 38ms/step - loss: 1.0154\n","Epoch 21/50\n","1/1 [==============================] - 0s 38ms/step - loss: 0.9794\n","Epoch 22/50\n","1/1 [==============================] - 0s 34ms/step - loss: 0.9408\n","Epoch 23/50\n","1/1 [==============================] - 0s 37ms/step - loss: 0.9002\n","Epoch 24/50\n","1/1 [==============================] - 0s 36ms/step - loss: 0.8585\n","Epoch 25/50\n","1/1 [==============================] - 0s 36ms/step - loss: 0.8166\n","Epoch 26/50\n","1/1 [==============================] - 0s 38ms/step - loss: 0.7753\n","Epoch 27/50\n","1/1 [==============================] - 0s 36ms/step - loss: 0.7352\n","Epoch 28/50\n","1/1 [==============================] - 0s 35ms/step - loss: 0.6965\n","Epoch 29/50\n","1/1 [==============================] - 0s 37ms/step - loss: 0.6594\n","Epoch 30/50\n","1/1 [==============================] - 0s 34ms/step - loss: 0.6237\n","Epoch 31/50\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5894\n","Epoch 32/50\n","1/1 [==============================] - 0s 34ms/step - loss: 0.5564\n","Epoch 33/50\n","1/1 [==============================] - 0s 38ms/step - loss: 0.5247\n","Epoch 34/50\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4942\n","Epoch 35/50\n","1/1 [==============================] - 0s 43ms/step - loss: 0.4651\n","Epoch 36/50\n","1/1 [==============================] - 0s 34ms/step - loss: 0.4372\n","Epoch 37/50\n","1/1 [==============================] - 0s 32ms/step - loss: 0.4106\n","Epoch 38/50\n","1/1 [==============================] - 0s 36ms/step - loss: 0.3852\n","Epoch 39/50\n","1/1 [==============================] - 0s 33ms/step - loss: 0.3609\n","Epoch 40/50\n","1/1 [==============================] - 0s 34ms/step - loss: 0.3376\n","Epoch 41/50\n","1/1 [==============================] - 0s 38ms/step - loss: 0.3152\n","Epoch 42/50\n","1/1 [==============================] - 0s 34ms/step - loss: 0.2937\n","Epoch 43/50\n","1/1 [==============================] - 0s 41ms/step - loss: 0.2730\n","Epoch 44/50\n","1/1 [==============================] - 0s 35ms/step - loss: 0.2533\n","Epoch 45/50\n","1/1 [==============================] - 0s 37ms/step - loss: 0.2344\n","Epoch 46/50\n","1/1 [==============================] - 0s 33ms/step - loss: 0.2166\n","Epoch 47/50\n","1/1 [==============================] - 0s 36ms/step - loss: 0.1996\n","Epoch 48/50\n","1/1 [==============================] - 0s 39ms/step - loss: 0.1837\n","Epoch 49/50\n","1/1 [==============================] - 0s 43ms/step - loss: 0.1689\n","Epoch 50/50\n","1/1 [==============================] - 0s 36ms/step - loss: 0.1551\n","1/1 [==============================] - 0s 230ms/step\n","Validation R-squared: -18.971036327472422\n","Epoch 1/50\n","2/2 [==============================] - 2s 14ms/step - loss: 12.3586\n","Epoch 2/50\n","2/2 [==============================] - 0s 13ms/step - loss: 10.9238\n","Epoch 3/50\n","2/2 [==============================] - 0s 13ms/step - loss: 9.9387\n","Epoch 4/50\n","2/2 [==============================] - 0s 14ms/step - loss: 8.9706\n","Epoch 5/50\n","2/2 [==============================] - 0s 14ms/step - loss: 8.0219\n","Epoch 6/50\n","2/2 [==============================] - 0s 13ms/step - loss: 7.1015\n","Epoch 7/50\n","2/2 [==============================] - 0s 12ms/step - loss: 6.2275\n","Epoch 8/50\n","2/2 [==============================] - 0s 12ms/step - loss: 5.4271\n","Epoch 9/50\n","2/2 [==============================] - 0s 15ms/step - loss: 4.7316\n","Epoch 10/50\n","2/2 [==============================] - 0s 13ms/step - loss: 4.1661\n","Epoch 11/50\n","2/2 [==============================] - 0s 14ms/step - loss: 3.7419\n","Epoch 12/50\n","2/2 [==============================] - 0s 12ms/step - loss: 3.4564\n","Epoch 13/50\n","2/2 [==============================] - 0s 12ms/step - loss: 3.2940\n","Epoch 14/50\n","2/2 [==============================] - 0s 13ms/step - loss: 3.2303\n","Epoch 15/50\n","2/2 [==============================] - 0s 13ms/step - loss: 3.2389\n","Epoch 16/50\n","2/2 [==============================] - 0s 13ms/step - loss: 3.2968\n","Epoch 17/50\n","2/2 [==============================] - 0s 13ms/step - loss: 3.3811\n","Epoch 18/50\n","2/2 [==============================] - 0s 13ms/step - loss: 3.4682\n","Epoch 19/50\n","2/2 [==============================] - 0s 12ms/step - loss: 3.5385\n","Epoch 20/50\n","2/2 [==============================] - 0s 20ms/step - loss: 3.5794\n","Epoch 21/50\n","2/2 [==============================] - 0s 12ms/step - loss: 3.5834\n","Epoch 22/50\n","2/2 [==============================] - 0s 13ms/step - loss: 3.5472\n","Epoch 23/50\n","2/2 [==============================] - 0s 16ms/step - loss: 3.4707\n","Epoch 24/50\n","2/2 [==============================] - 0s 13ms/step - loss: 3.3570\n","Epoch 25/50\n","2/2 [==============================] - 0s 12ms/step - loss: 3.2116\n","Epoch 26/50\n","2/2 [==============================] - 0s 12ms/step - loss: 3.0423\n","Epoch 27/50\n","2/2 [==============================] - 0s 13ms/step - loss: 2.8592\n","Epoch 28/50\n","2/2 [==============================] - 0s 12ms/step - loss: 2.6713\n","Epoch 29/50\n","2/2 [==============================] - 0s 12ms/step - loss: 2.4845\n","Epoch 30/50\n","2/2 [==============================] - 0s 13ms/step - loss: 2.3021\n","Epoch 31/50\n","2/2 [==============================] - 0s 14ms/step - loss: 2.1271\n","Epoch 32/50\n","2/2 [==============================] - 0s 14ms/step - loss: 1.9623\n","Epoch 33/50\n","2/2 [==============================] - 0s 13ms/step - loss: 1.8100\n","Epoch 34/50\n","2/2 [==============================] - 0s 16ms/step - loss: 1.6715\n","Epoch 35/50\n","2/2 [==============================] - 0s 14ms/step - loss: 1.5469\n","Epoch 36/50\n","2/2 [==============================] - 0s 12ms/step - loss: 1.4352\n","Epoch 37/50\n","2/2 [==============================] - 0s 14ms/step - loss: 1.3345\n","Epoch 38/50\n","2/2 [==============================] - 0s 13ms/step - loss: 1.2429\n","Epoch 39/50\n","2/2 [==============================] - 0s 12ms/step - loss: 1.1584\n","Epoch 40/50\n","2/2 [==============================] - 0s 13ms/step - loss: 1.0795\n","Epoch 41/50\n","2/2 [==============================] - 0s 14ms/step - loss: 1.0052\n","Epoch 42/50\n","2/2 [==============================] - 0s 15ms/step - loss: 0.9346\n","Epoch 43/50\n","2/2 [==============================] - 0s 20ms/step - loss: 0.8675\n","Epoch 44/50\n","2/2 [==============================] - 0s 22ms/step - loss: 0.8035\n","Epoch 45/50\n","2/2 [==============================] - 0s 19ms/step - loss: 0.7425\n","Epoch 46/50\n","2/2 [==============================] - 0s 19ms/step - loss: 0.6845\n","Epoch 47/50\n","2/2 [==============================] - 0s 17ms/step - loss: 0.6295\n","Epoch 48/50\n","2/2 [==============================] - 0s 15ms/step - loss: 0.5778\n","Epoch 49/50\n","2/2 [==============================] - 0s 16ms/step - loss: 0.5294\n","Epoch 50/50\n","2/2 [==============================] - 0s 15ms/step - loss: 0.4847\n","1/1 [==============================] - 0s 293ms/step\n","\n","Best hyperparameters: LSTM units: 64, Epochs: 50, Batch size: 32, Time steps: 10\n","Test R-squared with best hyperparameters: 0.747716839238484\n","Test mse with best hyperparameters: 0.003005668871486755\n"]}]},{"cell_type":"code","source":["# Best hyperparameters: LSTM units: 64, Epochs: 50, Batch size: 32, Time steps: 10\n","# Test R-squared with best hyperparameters: 0.8254605376536703\n","# Test mse with best hyperparameters: 0.002079440527211053\n","\n","# #nadam,\n","# Best hyperparameters: LSTM units: 64, Epochs: 50, Batch size: 32, Time steps: 10\n","# Test R-squared with best hyperparameters: 0.747716839238484\n","# Test mse with best hyperparameters: 0.003005668871486755\n","\n","print(y_test[best_time_steps:])\n","print(y_test_pred)"],"metadata":{"id":"VwZBDbRPq0We","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690737974457,"user_tz":-210,"elapsed":544,"user":{"displayName":"samira sheikha","userId":"14992994802119851005"}},"outputId":"5fcb814e-44e9-4d56-9327-a5efa08ebbe5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[4.04305127 4.12874599 4.1941899  4.24706565 4.27944005 4.33336146\n"," 4.38327585]\n","[[4.1497254]\n"," [4.198667 ]\n"," [4.2398787]\n"," [4.2827725]\n"," [4.306782 ]\n"," [4.326506 ]\n"," [4.35849  ]]\n"]}]},{"cell_type":"markdown","source":["#All Feature\n","the result are different from when you run the exact model on one variable, may be its because of the dence layer"],"metadata":{"id":"2HT-rX6IHrBG"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score,mean_squared_error\n","import tensorflow as tf\n","from keras.preprocessing.sequence import TimeseriesGenerator\n","\n","tf.keras.utils.set_random_seed(2023)\n","df=economic_df\n","# Modify the list of columns accordingly to include all your features\n","features_columns = [col for col in df.columns]\n","\n","# Separate features and target variable (if you have one) in your dataset\n","X = df[features_columns].values\n","\n","y = df[features_columns].values # Replace 'target' with the name of your target column\n","\n","# Split the data into train, test, and validation sets\n","X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)\n","X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=(2/3),shuffle=False)\n","print(len(X_train),len(X_val),len(X_test))\n","# Reshape input data to fit the LSTM model\n","num_features = len(features_columns)\n","\n","# Grid search hyperparameters\n","best_r_squared = -np.inf\n","best_lstm_units = 0\n","best_epochs = 0\n","best_batch_size = 0\n","best_time_steps = 0\n","\n","# lstm_units_list = [32, 64, 128]\n","# epochs_list = [50, 100, 150]\n","# batch_size_list = [16, 32, 64]\n","# time_steps_list = [3, 5, 7]\n","lstm_units_list = [64]\n","epochs_list = [50]\n","batch_size_list = [32]\n","time_steps_list = [10]\n","# X_train.shape[0]\n","for lstm_units in lstm_units_list:\n","    for epochs in epochs_list:\n","        for batch_size in batch_size_list:\n","            for time_steps in time_steps_list:\n","                print(f\"Training LSTM units: {lstm_units}, Epochs: {epochs}, Batch size: {batch_size}, Time steps: {time_steps}\")\n","\n","                # Reshape input data to fit the LSTM model\n","                X_train_lstm = TimeseriesGenerator(X_train, y_train, length=time_steps, sampling_rate=1)\n","                X_val_lstm = TimeseriesGenerator(X_val, y_val, length=time_steps, sampling_rate=1)\n","\n","                # Create the LSTM model\n","                model = Sequential()\n","                model.add(LSTM(lstm_units, activation='sigmoid', input_shape=(time_steps, num_features)))\n","                model.add(Dense(num_features)) # Output layer with one node for regression, adjust for classification\n","\n","                # Compile the model\n","                model.compile(optimizer='Nadam', loss='mse') # Use appropriate loss for your task\n","\n","                # Train the model without shuffling\n","                model.fit(X_train_lstm, epochs=epochs,batch_size=batch_size, shuffle=False)\n","\n","                # Evaluate the model on the validation set\n","                y_val_pred = model.predict(X_val_lstm)\n","                r_squared = r2_score(y_val[time_steps:], y_val_pred)\n","                print(f\"Validation R-squared: {r_squared}\")\n","\n","                # Update best hyperparameters if necessary\n","                if r_squared > best_r_squared:\n","                    best_r_squared = r_squared\n","                    best_lstm_units = lstm_units\n","                    best_epochs = epochs\n","                    best_batch_size = batch_size\n","                    best_time_steps = time_steps\n","\n","# Train the best model on the combined training and validation set\n","X_train_combined = np.concatenate((X_train, X_val), axis=0)\n","y_train_combined = np.concatenate((y_train, y_val), axis=0)\n","X_train_combined_lstm = TimeseriesGenerator(X_train_combined, y_train_combined , length=best_time_steps, sampling_rate=1)\n","\n","model = Sequential()\n","model.add(LSTM(best_lstm_units, activation='sigmoid', input_shape=(best_time_steps, num_features)))\n","model.add(Dense(num_features)) # Output layer with one node for regression, adjust for classification\n","model.compile(optimizer='Nadam', loss='mse')\n","model.fit(X_train_combined_lstm, epochs=best_epochs, batch_size=best_batch_size, shuffle=False)\n","\n","# Evaluate the best model on the test set\n","X_test_lstm = TimeseriesGenerator(X_test, y_test , length=best_time_steps, sampling_rate=1)\n","y_test_pred = model.predict(X_test_lstm)\n","test_r_squared = r2_score(y_test[best_time_steps:], y_test_pred)\n","test_mse = mean_squared_error(y_test[best_time_steps:], y_test_pred)\n","print(f\"\\nBest hyperparameters: LSTM units: {best_lstm_units}, Epochs: {best_epochs}, Batch size: {best_batch_size}, Time steps: {best_time_steps}\")\n","print(f\"Test R-squared with best hyperparameters: {test_r_squared}\")\n","print(f\"Test mse with best hyperparameters: {test_mse}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ohJ7jLgXB-vS","executionInfo":{"status":"ok","timestamp":1690738483763,"user_tz":-210,"elapsed":15049,"user":{"displayName":"samira sheikha","userId":"14992994802119851005"}},"outputId":"f04138ba-2d24-4bbd-c568-0841f8cc483a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["117 34 17\n","Training LSTM units: 64, Epochs: 50, Batch size: 32, Time steps: 10\n","Epoch 1/50\n","1/1 [==============================] - 3s 3s/step - loss: 40.1031\n","Epoch 2/50\n","1/1 [==============================] - 0s 152ms/step - loss: 39.4725\n","Epoch 3/50\n","1/1 [==============================] - 0s 116ms/step - loss: 39.0266\n","Epoch 4/50\n","1/1 [==============================] - 0s 169ms/step - loss: 38.5674\n","Epoch 5/50\n","1/1 [==============================] - 0s 85ms/step - loss: 38.0926\n","Epoch 6/50\n","1/1 [==============================] - 0s 92ms/step - loss: 37.6022\n","Epoch 7/50\n","1/1 [==============================] - 0s 114ms/step - loss: 37.0959\n","Epoch 8/50\n","1/1 [==============================] - 0s 163ms/step - loss: 36.5736\n","Epoch 9/50\n","1/1 [==============================] - 0s 110ms/step - loss: 36.0348\n","Epoch 10/50\n","1/1 [==============================] - 0s 105ms/step - loss: 35.4792\n","Epoch 11/50\n","1/1 [==============================] - 0s 162ms/step - loss: 34.9065\n","Epoch 12/50\n","1/1 [==============================] - 0s 195ms/step - loss: 34.3166\n","Epoch 13/50\n","1/1 [==============================] - 0s 173ms/step - loss: 33.7094\n","Epoch 14/50\n","1/1 [==============================] - 0s 139ms/step - loss: 33.0856\n","Epoch 15/50\n","1/1 [==============================] - 0s 85ms/step - loss: 32.4461\n","Epoch 16/50\n","1/1 [==============================] - 0s 104ms/step - loss: 31.7919\n","Epoch 17/50\n","1/1 [==============================] - 0s 79ms/step - loss: 31.1245\n","Epoch 18/50\n","1/1 [==============================] - 0s 34ms/step - loss: 30.4453\n","Epoch 19/50\n","1/1 [==============================] - 0s 36ms/step - loss: 29.7558\n","Epoch 20/50\n","1/1 [==============================] - 0s 34ms/step - loss: 29.0575\n","Epoch 21/50\n","1/1 [==============================] - 0s 39ms/step - loss: 28.3528\n","Epoch 22/50\n","1/1 [==============================] - 0s 37ms/step - loss: 27.6454\n","Epoch 23/50\n","1/1 [==============================] - 0s 35ms/step - loss: 26.9398\n","Epoch 24/50\n","1/1 [==============================] - 0s 37ms/step - loss: 26.2420\n","Epoch 25/50\n","1/1 [==============================] - 0s 39ms/step - loss: 25.5573\n","Epoch 26/50\n","1/1 [==============================] - 0s 37ms/step - loss: 24.8894\n","Epoch 27/50\n","1/1 [==============================] - 0s 35ms/step - loss: 24.2404\n","Epoch 28/50\n","1/1 [==============================] - 0s 33ms/step - loss: 23.6105\n","Epoch 29/50\n","1/1 [==============================] - 0s 38ms/step - loss: 22.9995\n","Epoch 30/50\n","1/1 [==============================] - 0s 36ms/step - loss: 22.4068\n","Epoch 31/50\n","1/1 [==============================] - 0s 37ms/step - loss: 21.8321\n","Epoch 32/50\n","1/1 [==============================] - 0s 35ms/step - loss: 21.2753\n","Epoch 33/50\n","1/1 [==============================] - 0s 41ms/step - loss: 20.7361\n","Epoch 34/50\n","1/1 [==============================] - 0s 38ms/step - loss: 20.2147\n","Epoch 35/50\n","1/1 [==============================] - 0s 34ms/step - loss: 19.7111\n","Epoch 36/50\n","1/1 [==============================] - 0s 36ms/step - loss: 19.2254\n","Epoch 37/50\n","1/1 [==============================] - 0s 37ms/step - loss: 18.7575\n","Epoch 38/50\n","1/1 [==============================] - 0s 40ms/step - loss: 18.3067\n","Epoch 39/50\n","1/1 [==============================] - 0s 43ms/step - loss: 17.8724\n","Epoch 40/50\n","1/1 [==============================] - 0s 43ms/step - loss: 17.4534\n","Epoch 41/50\n","1/1 [==============================] - 0s 36ms/step - loss: 17.0486\n","Epoch 42/50\n","1/1 [==============================] - 0s 38ms/step - loss: 16.6567\n","Epoch 43/50\n","1/1 [==============================] - 0s 34ms/step - loss: 16.2771\n","Epoch 44/50\n","1/1 [==============================] - 0s 37ms/step - loss: 15.9100\n","Epoch 45/50\n","1/1 [==============================] - 0s 36ms/step - loss: 15.5562\n","Epoch 46/50\n","1/1 [==============================] - 0s 36ms/step - loss: 15.2162\n","Epoch 47/50\n","1/1 [==============================] - 0s 33ms/step - loss: 14.8900\n","Epoch 48/50\n","1/1 [==============================] - 0s 32ms/step - loss: 14.5763\n","Epoch 49/50\n","1/1 [==============================] - 0s 35ms/step - loss: 14.2734\n","Epoch 50/50\n","1/1 [==============================] - 0s 41ms/step - loss: 13.9795\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f381384f130> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 203ms/step\n","Validation R-squared: -978.13185058959\n","Epoch 1/50\n","2/2 [==============================] - 2s 13ms/step - loss: 56.4199\n","Epoch 2/50\n","2/2 [==============================] - 0s 14ms/step - loss: 55.2289\n","Epoch 3/50\n","2/2 [==============================] - 0s 13ms/step - loss: 54.2290\n","Epoch 4/50\n","2/2 [==============================] - 0s 14ms/step - loss: 53.1608\n","Epoch 5/50\n","2/2 [==============================] - 0s 14ms/step - loss: 52.0236\n","Epoch 6/50\n","2/2 [==============================] - 0s 14ms/step - loss: 50.8165\n","Epoch 7/50\n","2/2 [==============================] - 0s 14ms/step - loss: 49.5340\n","Epoch 8/50\n","2/2 [==============================] - 0s 13ms/step - loss: 48.1655\n","Epoch 9/50\n","2/2 [==============================] - 0s 13ms/step - loss: 46.7046\n","Epoch 10/50\n","2/2 [==============================] - 0s 13ms/step - loss: 45.1629\n","Epoch 11/50\n","2/2 [==============================] - 0s 14ms/step - loss: 43.5672\n","Epoch 12/50\n","2/2 [==============================] - 0s 13ms/step - loss: 41.9483\n","Epoch 13/50\n","2/2 [==============================] - 0s 13ms/step - loss: 40.3378\n","Epoch 14/50\n","2/2 [==============================] - 0s 19ms/step - loss: 38.7688\n","Epoch 15/50\n","2/2 [==============================] - 0s 14ms/step - loss: 37.2734\n","Epoch 16/50\n","2/2 [==============================] - 0s 13ms/step - loss: 35.8724\n","Epoch 17/50\n","2/2 [==============================] - 0s 12ms/step - loss: 34.5748\n","Epoch 18/50\n","2/2 [==============================] - 0s 13ms/step - loss: 33.3809\n","Epoch 19/50\n","2/2 [==============================] - 0s 14ms/step - loss: 32.2868\n","Epoch 20/50\n","2/2 [==============================] - 0s 14ms/step - loss: 31.2823\n","Epoch 21/50\n","2/2 [==============================] - 0s 13ms/step - loss: 30.3548\n","Epoch 22/50\n","2/2 [==============================] - 0s 13ms/step - loss: 29.4926\n","Epoch 23/50\n","2/2 [==============================] - 0s 13ms/step - loss: 28.6851\n","Epoch 24/50\n","2/2 [==============================] - 0s 13ms/step - loss: 27.9215\n","Epoch 25/50\n","2/2 [==============================] - 0s 13ms/step - loss: 27.1919\n","Epoch 26/50\n","2/2 [==============================] - 0s 14ms/step - loss: 26.4871\n","Epoch 27/50\n","2/2 [==============================] - 0s 18ms/step - loss: 25.8007\n","Epoch 28/50\n","2/2 [==============================] - 0s 14ms/step - loss: 25.1291\n","Epoch 29/50\n","2/2 [==============================] - 0s 14ms/step - loss: 24.4696\n","Epoch 30/50\n","2/2 [==============================] - 0s 24ms/step - loss: 23.8182\n","Epoch 31/50\n","2/2 [==============================] - 0s 13ms/step - loss: 23.1714\n","Epoch 32/50\n","2/2 [==============================] - 0s 13ms/step - loss: 22.5290\n","Epoch 33/50\n","2/2 [==============================] - 0s 14ms/step - loss: 21.8947\n","Epoch 34/50\n","2/2 [==============================] - 0s 13ms/step - loss: 21.2780\n","Epoch 35/50\n","2/2 [==============================] - 0s 20ms/step - loss: 20.6905\n","Epoch 36/50\n","2/2 [==============================] - 0s 13ms/step - loss: 20.1339\n","Epoch 37/50\n","2/2 [==============================] - 0s 13ms/step - loss: 19.6036\n","Epoch 38/50\n","2/2 [==============================] - 0s 13ms/step - loss: 19.0954\n","Epoch 39/50\n","2/2 [==============================] - 0s 14ms/step - loss: 18.6066\n","Epoch 40/50\n","2/2 [==============================] - 0s 13ms/step - loss: 18.1349\n","Epoch 41/50\n","2/2 [==============================] - 0s 13ms/step - loss: 17.6788\n","Epoch 42/50\n","2/2 [==============================] - 0s 14ms/step - loss: 17.2367\n","Epoch 43/50\n","2/2 [==============================] - 0s 17ms/step - loss: 16.8075\n","Epoch 44/50\n","2/2 [==============================] - 0s 15ms/step - loss: 16.3897\n","Epoch 45/50\n","2/2 [==============================] - 0s 13ms/step - loss: 15.9819\n","Epoch 46/50\n","2/2 [==============================] - 0s 13ms/step - loss: 15.5825\n","Epoch 47/50\n","2/2 [==============================] - 0s 13ms/step - loss: 15.1909\n","Epoch 48/50\n","2/2 [==============================] - 0s 15ms/step - loss: 14.8080\n","Epoch 49/50\n","2/2 [==============================] - 0s 14ms/step - loss: 14.4344\n","Epoch 50/50\n","2/2 [==============================] - 0s 13ms/step - loss: 14.0693\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3817dca7a0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 198ms/step\n","\n","Best hyperparameters: LSTM units: 64, Epochs: 50, Batch size: 32, Time steps: 10\n","Test R-squared with best hyperparameters: -2289.668134128751\n","Test mse with best hyperparameters: 22.793804718239205\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0QlfjetJ0eaU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#to check the result\n","import statistics\n","y=y_test[best_time_steps:]\n","yh=y_test_pred\n","\n","z=[]\n","for i in range(len(y[0])):\n","  e=[]\n","  for j in range(len(y)):\n","    e.append(y[j][i])\n","  z.append(e)\n","print(z)\n","t=[]\n","for i in range(len(yh[0])):\n","  e=[]\n","  for j in range(len(yh)):\n","    e.append(yh[j][i])\n","  t.append(e)\n","print(t)\n","r=[]\n","for i in range(len(t)):\n","  # print(z[i],t[i])\n","  r.append(r2_score(z[i],t[i]))\n","print(r)\n","print(statistics.mean(r))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lO9O0qB6QJq5","executionInfo":{"status":"ok","timestamp":1690738563053,"user_tz":-210,"elapsed":24,"user":{"displayName":"samira sheikha","userId":"14992994802119851005"}},"outputId":"ce635dc9-3d4e-4391-afda-914fec283cd1"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[4.04305126783455, 4.128745988939433, 4.194189897191817, 4.247065649239764, 4.279440045898781, 4.333361462692601, 4.383275854074314], [15.343073530211534, 15.369336495499228, 15.437660232513014, 15.521656363854808, 15.671105926549018, 15.710708777441349, 15.770778457805696], [10.833329096043434, 10.7357172490612, 10.7580970107839, 10.736587635498076, 10.692653139321576, 10.938691348011186, 10.80022638567907], [4.283586561860629, 4.436751534363128, 4.511957804265912, 4.526126978647638, 4.540098189244376, 4.565389315976247, 4.5736795188967205], [8.624611588183507, 8.759511722116487, 8.825412915085566, 8.874168090363968, 8.904630097005011, 8.988446040062405, 9.019058793810718], [8.894670262984233, 8.937875265329263, 9.059168584174444, 9.088850474388472, 9.135401285311689, 9.29596745794362, 9.17378011666438], [9.414097209490203, 9.414097209490203, 10.056453244828615, 10.120496324513569, 10.121429599450872, 10.14680621078697, 10.17640744825082], [10.4691904904112, 10.476019565738097, 10.37682001007998, 10.308495889673647, 10.303598351882368, 10.381484152710788, 10.357164880058045], [9.54180006677385, 9.496706579056738, 9.282195809076612, 9.133275725473204, 9.118773178450692, 9.18898393814762, 9.159583775645725], [12.70734538545047, 12.536300426054058, 12.545151125803132, 12.528335476444475, 12.719153424521025, 12.577864796522585, 12.585470947276063], [4.120661870539474, 4.175924549214524, 4.2626798770413155, 4.3067641501733345, 4.334672938290411, 4.3528552573736015, 4.387014176184921], [4.109233174715851, 4.157319361383489, 4.235554730773624, 4.279440045898781, 4.310799125385514, 4.33204826486764, 4.373238128640803], [4.09933210373314, 4.194692536006132, 4.250398192147917, 4.285882774166851, 4.310799125385514, 4.344238228568954, 4.386599496560493]]\n","[[2.8121562, 2.812847, 2.8143547, 2.8139048, 2.814279, 2.814267, 2.815041], [5.8081007, 5.807606, 5.807173, 5.8066325, 5.806446, 5.80632, 5.8067985], [4.0372767, 4.0369697, 4.0381613, 4.036113, 4.036277, 4.03643, 4.037005], [2.9038603, 2.9046953, 2.906562, 2.908136, 2.9083612, 2.9072394, 2.9092307], [4.9921646, 4.9915276, 4.992022, 4.989609, 4.989785, 4.990413, 4.990416], [4.5690975, 4.570693, 4.573255, 4.5779634, 4.579634, 4.581325, 4.580132], [4.6808114, 4.6828275, 4.6843123, 4.6896763, 4.6907144, 4.691461, 4.6912837], [5.335363, 5.3361716, 5.336261, 5.337771, 5.337805, 5.337681, 5.338491], [5.340721, 5.341531, 5.3416443, 5.3427167, 5.343156, 5.3443065, 5.3434377], [6.0071135, 6.008051, 6.0077233, 6.00758, 6.007638, 6.0082088, 6.0083804], [3.443506, 3.4435148, 3.4448655, 3.4434001, 3.4433784, 3.442759, 3.443966], [3.571415, 3.5724375, 3.5744631, 3.5769002, 3.5773084, 3.576516, 3.5781038], [2.5400271, 2.5408223, 2.5398438, 2.5400221, 2.5396702, 2.5393336, 2.540084]]\n","-2289.668134128751\n","[-168.28999931962224, -3729.724421051772, -7926.898486096142, -281.4319523400906, -945.7328239974032, -1253.4147090302192, -263.2397003824484, -6221.26190271844, -583.1147447991956, -7875.880830206279, -85.44034654282326, -59.3044472448719, -371.95137994445565]\n"]},{"output_type":"execute_result","data":{"text/plain":["-168.28999931962224"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["[4.04305127 4.12874599 4.1941899  4.24706565 4.27944005 4.33336146 4.38327585]\n","[2.8121562, 2.812847, 2.8143547, 2.8139048, 2.814279, 2.814267, 2.815041]\n","[[4.1497254],[4.198667 ],[4.2398787],[4.2827725],[4.306782 ],[4.326506 ],[4.35849 ]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWkdlCzLYwP1","executionInfo":{"status":"ok","timestamp":1690730501223,"user_tz":-210,"elapsed":677,"user":{"displayName":"samira sheikha","userId":"14992994802119851005"}},"outputId":"8f0c61a0-4d83-4ee6-c87f-bb5c3743be3b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7857142857142857"]},"metadata":{},"execution_count":36}]}]}